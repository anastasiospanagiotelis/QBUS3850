{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style = \"fontsize:400%;text-align:center;\">QBUS3850: Time Series and Forecasting</h1>\n",
    "<h2 style = \"fontsize:300%;text-align:center;\">ARIMA</h2>\n",
    "<h3 style = \"fontsize:200%;text-align:center;\">Lecture Notes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style = \"fontsize:300%;text-align:center;\">Why ARIMA?</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ARIMA models\n",
    "\n",
    "- Have been a popular forecasting tool since at least the 1970s \n",
    "- Why do they work?\n",
    "  - Data can be differenced to make it stationary.\n",
    "  - Any stationary series has an ARMA representation.\n",
    "  - Maximum likelihood can be used to estimate ARMA models that can be used to generate forecasts\n",
    "- First question: What is stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style = \"fontsize:300%;text-align:center;\">Stationarity</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Expectations in time series\n",
    "\n",
    "- Consider the time series\n",
    "$$Y_1,Y_2,\\dots,Y_T$$\n",
    "- What do we mean by the expected value $E(Y_t)$?\n",
    "- Let the density of $Y_t$ be $f_{Y_t}$. The expected value is given by\n",
    "$$E(Y_t)=\\int y_tf_{Y_t}(y_t)dy_t$$\n",
    "- This is the *unconditional* expected value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Understanding expectations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"drstrange.jpeg\" alt=\"multiverse\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An Illustration\n",
    "\n",
    "- Imagine $I$ multiverses each with different realisations of the time series\n",
    "$$\\begin{aligned}\\textrm{Computer 1:}\\,&y^{(1)}_1,y^{(1)}_2\\dots, y^{(1)}_t,\\dots,y^{(1)}_{T}\\\\\\textrm{Computer 2:}\\,& y^{(2)}_1,y^{(2)}_2\\dots, y^{(2)}_t,\\dots,y^{(2)}_{T}\\\\&\\vdots\\quad\\vdots\\quad\\vdots\\quad\\vdots\\quad\\vdots\\quad\\vdots\\\\\\textrm{Computer T:}\\,& y^{(I)}_1,y^{(I)}_2\\dots, y^{(I)}_t,\\dots,y^{(I)}_{T}\\end{aligned}$$\n",
    "- Let the number of multiverses go to infinity\n",
    "- Take observation $t$ from each computer and average them.\n",
    "$$E(Y_t)=\\underset{I\\rightarrow\\infty}{\\lim}\\frac{1}{I}\\sum\\limits{}{}Y_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variance and (Auto-)Covariance in Time Series\n",
    "\n",
    "- The variance of $Y_t$ is given by\n",
    "$$V(Y_t)=E\\left[(y_t-\\mu_t)^2\\right]$$\n",
    "- Here $\\mu_t=E(Y_t)$ is just shorthand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The covariance between $Y_t$ and $Y_{t-k}$ is given by\n",
    "$$\\textrm{Cov}(Y_t,T_{t-k})=E\\left[(y_t-\\mu_t)(y_{t-k}-\\mu_{t-k})\\right]$$\n",
    "- Together, the variance and covariance make up the first two *moments* of the time series.\n",
    "- Both can be defined using an integral, but also via the \"multiverses\" thought experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variance Covariance Matrix\n",
    "\n",
    "It is often convenient to put all variances and coavariances into a matrix\n",
    "\n",
    "$$\\textrm{Var-Cov}(\\mathbf{Y})=\\begin{bmatrix}\\textrm{Var}(Y_1) & \\textrm{Cov}(Y_1,Y_2) &\\cdots& \\textrm{Cov}(Y_1,Y_T) \\\\ \\textrm{Cov}(Y_1,Y_2)&\\textrm{Var}(Y_2) &\\cdots& \\textrm{Cov}(Y_2,Y_T)\\\\\\vdots&\\vdots&\\ddots&\\vdots \\\\\\textrm{Cov}(Y_1,Y_T)&\\textrm{Cov}(Y_2,Y_T) &\\cdots& \\textrm{Var}(Y_T)\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style = \"fontsize:300%;text-align:center;\">Playing with lag operators</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style = \"fontsize:300%;text-align:center;\">Box Jenkins</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style = \"fontsize:300%;text-align:center;\">Auto ARIMA</h2>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
