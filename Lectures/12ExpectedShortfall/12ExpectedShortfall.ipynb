{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style = \"fontsize:400%;text-align:center;\">QBUS3850: Time Series and Forecasting</h1>\n",
    "<h2 style = \"fontsize:300%;text-align:center;\">Expected Shortfall</h2>\n",
    "<h3 style = \"fontsize:200%;text-align:center;\">Lecture Notes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style = \"fontsize:300%;text-align:center;\">Problems with VaR</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Which is riskier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  - Return of -2 with probability 0.005\n",
    "  - Return of -1 with probability 0.005\n",
    "  - Return of 0 with probability 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  - Return of -100 with probability 0.005\n",
    "  - Return of -1 with probability 0.005\n",
    "  - Return of 0 with probability 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is the 1% VaR of each investment (define quantile as $q:\\textrm{Pr}(R\\leq q)=\\alpha$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value at Risk\n",
    "\n",
    "- One shortcoming of Value at Risk is that it only accounts for the *minimum* amount lost on 1 out of 100 (or 1 out of 20, 1 out of 1000, etc.) days.\n",
    "- It does not account for how much could be lost when there is a violation.\n",
    "- In the example above, both assets have the same VaR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Two more assets\n",
    "\n",
    "- Consider two assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  - Political Crisis: Return of -3 with probability 0.005\n",
    "  - Financial Crisis: Return of -1 with probability 0.005\n",
    "  - Good economy: Return of 0 with probability 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  - Political Crisis: Return of -1 with probability 0.005\n",
    "  - Financial Crisis: Return of -9 with probability 0.005\n",
    "  - Good economy: Return of 0 with probability 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is the 1% VaR of each investment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Portfolio\n",
    "\n",
    "- Imagine you invest in both assets ($X+Y$).\n",
    "\n",
    "  - Political Crisis: Return of -4 with probability 0.005\n",
    "  - Financial Crisis: Return of -10 with probability 0.005\n",
    "  - Good economy: Return of 0 with probability 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is the 1% VaR of the sum of both assets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Does this make sense when compared to the results for the single assets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sub-additivity\n",
    "\n",
    "- Diversification implies that portfolios should be less risky.\n",
    "- For a risk measure $\\rho$ and two assets $X$ and $Y$ this implies that\n",
    "$$\\rho(X)+\\rho(Y)\\geq \\rho(X+Y)$$\n",
    "- A portfolio should be at most as risky (or less risky) than the sum of riskiness of each asset alone.\n",
    "- The previous examples show that Value at Risk **may not** be subadditive.\n",
    "- This does not mean that there are not cases where VaR is subadditive (e.g. normality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style = \"fontsize:300%;text-align:center;\">Expected Shortfall</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Expected Shortfall\n",
    "\n",
    "- Expected Shortfall is given by.\n",
    "\n",
    "$$ES^{(\\alpha)}(X)=\\frac{1}{\\alpha}\\int_0^\\alpha VaR^{(u)}du$$\n",
    "\n",
    "- This is the same as the expected loss given that there is a violation, i.e.\n",
    "\n",
    "$$ES^{(\\alpha)}(X)=E(X|X<VaR^{(\\alpha)})$$\n",
    "\n",
    "- This is also called conditional VaR, tail VaR, tail expectation or tail risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Which is riskier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  - Return of -2 with probability 0.005\n",
    "  - Return of -1 with probability 0.005\n",
    "  - Return of 0 with probability 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  - Return of -100 with probability 0.005\n",
    "  - Return of -1 with probability 0.005\n",
    "  - Return of 0 with probability 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is the 1% ES of each investment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Two more assets\n",
    "\n",
    "- Consider the same two assets as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  - Political Crisis: Return of -3 with probability 0.005\n",
    "  - Financial Crisis: Return of -1 with probability 0.005\n",
    "  - Good economy: Return of 0 with probability 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  - Political Crisis: Return of -1 with probability 0.005\n",
    "  - Financial Crisis: Return of -9 with probability 0.005\n",
    "  - Good economy: Return of 0 with probability 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is the 1% ES of each investment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Portfolio\n",
    "\n",
    "- Imagine you invest in both assets ($X+Y$).\n",
    "\n",
    "  - Political Crisis: Return of -4 with probability 0.005\n",
    "  - Financial Crisis: Return of -10 with probability 0.005\n",
    "  - Good economy: Return of 0 with probability 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is the 1% ES of the sum of both assets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How does this compare to the sum of ES of the individual assets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Subadditivity of ES\n",
    "\n",
    "- This one example is not a proof.\n",
    "- However the subadditivity of ES has been rigorously proven.\n",
    "- *Coherent* risk measures have a number of properties that follow commonsense\n",
    "  - Subadditivity is one of these\n",
    "- ES also satisfies the other properties (as does VaR)\n",
    "- For this reason Basel III (the successor to Basel II) prefers ES over VaR as a risk measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style = \"fontsize:300%;text-align:center;\">Problems with ES</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Elicitability of a risk measure\n",
    "\n",
    "- It is necessary to evaluate whether forecasts of a risk measure are accurate.\n",
    "- Recall for VaR we could use the check loss function\n",
    "- Why could we use this?\n",
    "- This requires an understanding of the role of scoring rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scoring rule\n",
    "\n",
    "- For a random variable $X$ and a 'forecast' $v$, a scoring rule is any function $S(v,x)$ that takes $x$ (the realised value of $X$) and $v$ as inputs and returns a single number.\n",
    "- Smaller values of the 'score' indicate better forecasts.\n",
    "- A good property for the scoring rule to have is\n",
    "$$\\rho=\\underset{v}{argmin}E_X(S(v,x))$$\n",
    "- Where $\\rho$ is our risk measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why is  this a good property?\n",
    "\n",
    "- Suppose $\\rho$ is the true VaR (i.e. a quantile).\n",
    "- The 'forecast' that gives the smallest score (on average) will give us the true value of the risk measure.\n",
    "- In practice we can evaluate the score over a rolling window\n",
    "  - Methods with smaller average values of the score are give something closer to the true value of the risk measure. \n",
    "- For VaR, one example of a scoring rule is the check loss (covered last week).\n",
    "- The check loss makes the VaR an *elicitable* risk measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ES is not elicitable\n",
    "\n",
    "- It has been proven that ES in not elicitable.\n",
    "- There is no objective function that can be minimised to get the 'true' ES value.\n",
    "- Even if we knew the true ES, there is not score that would be guaranteed to show that the true ES is better than some other ES.\n",
    "- This is a weakness in evaluating different ES forecasts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What to do?\n",
    "\n",
    "- Let $r_t$ be returns and $ES^{(\\alpha)}_{t|t-h}$ be $\\alpha$ level ES forecasts.\n",
    "- Compute $\\xi_t=r_t-ES^{(\\alpha)}_{t|t-h}$ using **only** violations.\n",
    "- Do a t-test that the mean of $\\xi_t=0$\n",
    "- Do a t-test that the mean of $\\xi_t/\\hat{\\sigma}_{t|t-h}=0$\n",
    "- Use RMSE and MAD on $\\xi_t=0$ and $\\xi_t/\\hat{\\sigma}_{t|t-h}=0$ to compare different methods.\n",
    "- The tests in particular are generally weak or based on invalid assumptions of independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Joint elicitability\n",
    "\n",
    "- Fissler and Ziegel, published a paper in 2016 showing that ES and VaR are jointly elicitable.\n",
    "- The function needs to take ES, VaR and the value of returns as an input.\n",
    "- The function is\n",
    "$$S(v,e,x)=(I(x\\leq v)-\\alpha)(G_1(v)-G_1(x))+\\frac{1}{\\alpha}G_2(e)I(x<v)(v-x)+G_2(e)(e-v)-\\mathcal{G}_2(e)$$\n",
    "\n",
    "- where $G_1,G_2$ are monotonic and $\\mathcal{G}'_2=G_2$.\n",
    "- choices that work are $G_1(v)=v$ and $G_2(e)=\\mathcal{G}_2(e)=exp(e)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In practice\n",
    "\n",
    "- Compute VaR and ES for method A and method B\n",
    "- Compute scores for both methods\n",
    "- Carry out a Diebold Mariano test (a version of a two-sample t-test for serially correlated data) to test whether there are differences between the methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style = \"fontsize:300%;text-align:center;\">Forecasting ES</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ES for 1-step ahead\n",
    "\n",
    "- For 1-step ahead forecasts and for Gaussian errors the ES is given by\n",
    "\n",
    "$$\\hat{\\mu}_{t|t-1}-\\hat{\\sigma}_{t|t-1}\\frac{\\phi(\\Phi^{-1}(\\alpha))}{\\alpha}$$\n",
    "\n",
    "- Here $\\phi$ is the density (pdf) and $\\Phi^{-1}(.)$ is the inverse cdf of a standard normal.\n",
    "- For 1-step ahead forecasts and for t errors the ES is given by\n",
    "\n",
    "$$\\hat{\\mu}_{t|t-1}-\\hat{\\sigma}_{t|t-1}\\frac{t_{\\nu}(T_{\\nu}^{-1}(\\alpha))}{\\alpha}\\frac{\\nu + (T^{-1}_{\\nu}(\\alpha))^2}{\\nu-1}\\sqrt{\\frac{\\nu-2}{\\nu}}$$\n",
    "\n",
    "- Here $t_{\\nu}$ is the pdf and $T_{\\nu}^{-1}(.)$ is the inverse cdf of a t-distribution with $\\nu$ df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ES for multi-step ahead\n",
    "\n",
    "- These formulas no longer work for h-step ahead forecasts.\n",
    "- Instead Monte Carlo simulation can be used\n",
    "- Simulate a path of future values by\n",
    "  - Simulate $\\tilde{r}_{t+1}$ conditional on all observed $r_t$ \n",
    "  - Simulate $\\tilde{r}_{t+2}$ conditional on all observed $r_t$ and $\\tilde{r}_{t+1}$ as if it were $r_{t+1}$\n",
    "  - Continue until simulating $\\tilde{r}_{t+h}$\n",
    "- Repeat this $B$ times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ES for multi-step ahead\n",
    "\n",
    "- Take the $B$ values of $\\tilde{r}_{t+h}$\n",
    "- Select only the lowest 5\\% (or more generally $100\\alpha$ \\%) of values.\n",
    "- Take the mean.\n",
    "- This is a Monte Carlo estimate of ES\n",
    "- Often $B$ needs to be big to ensure stable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "bhp = pd.read_csv('BHP.AX.csv')\n",
    "bhp['Date'] = pd.to_datetime(bhp['Date'])\n",
    "ret=np.log(bhp['Close']).diff()[1:]\n",
    "from arch import arch_model\n",
    "Teval = 400\n",
    "alpha = 0.05\n",
    "B=1000\n",
    "q = scipy.stats.norm.ppf(alpha)\n",
    "e = scipy.stats.norm.pdf(q)/alpha\n",
    "Actual = ret.tail(Teval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One step ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Sigma_GARCH = np.zeros(Teval)\n",
    "VaR_GARCH = np.zeros(Teval)\n",
    "ES_GARCH = np.zeros(Teval)\n",
    "for j in range(Teval):\n",
    "    ret_train = ret[:-Teval+j]\n",
    "    garch = arch_model(ret_train,mean='Constant',vol='GARCH',p=1,q=1)\n",
    "    garchfit=garch.fit(disp='off')\n",
    "    fc=garchfit.forecast(horizon = 1)\n",
    "    Sigma_GARCH[j] = np.sqrt(fc.variance['h.1'].tail(1))\n",
    "    VaR_GARCH[j] = fc.mean['h.1'].tail(1) + Sigma_GARCH[j] * q\n",
    "    ES_GARCH[j] = fc.mean['h.1'].tail(1) - Sigma_GARCH[j] * e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One step ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.5288859008430489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06996520957629498"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi = (Actual-ES_GARCH)[Actual<VaR_GARCH]\n",
    "t = np.mean(xi)/(np.std(xi)/np.sqrt(len(xi)))\n",
    "print(t)\n",
    "scipy.stats.t.cdf(t,df = len(xi-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unable to reject validity of model at 5% level of significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One step ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.6881217861873286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0524516144683017"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi = (Actual-ES_GARCH)[Actual<VaR_GARCH]/Sigma_GARCH[Actual<VaR_GARCH]\n",
    "t = np.mean(xi)/(np.std(xi)/np.sqrt(len(xi)))\n",
    "print(t)\n",
    "scipy.stats.t.cdf(t,df = len(xi-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unable to reject validity of model at 5% level of significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Same for ARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Sigma_ARCH = np.zeros(Teval)\n",
    "VaR_ARCH = np.zeros(Teval)\n",
    "ES_ARCH = np.zeros(Teval)\n",
    "for j in range(Teval):\n",
    "    ret_train = ret[:-Teval+j]\n",
    "    arch = arch_model(ret_train,mean='Constant',vol='ARCH',p=1)\n",
    "    archfit=arch.fit(disp='off')\n",
    "    fc=archfit.forecast(horizon = 1)\n",
    "    Sigma_ARCH[j] = np.sqrt(fc.variance['h.1'].tail(1))\n",
    "    VaR_ARCH[j] = fc.mean['h.1'].tail(1) + Sigma_ARCH[j] * q\n",
    "    ES_ARCH[j] = fc.mean['h.1'].tail(1) - Sigma_ARCH[j] * e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One step ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.3475478416946887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09774509238987435"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi = (Actual-ES_ARCH)[Actual<VaR_ARCH]\n",
    "t = np.mean(xi)/(np.std(xi)/np.sqrt(len(xi)))\n",
    "print(t)\n",
    "scipy.stats.t.cdf(t,df = len(xi-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unable to reject validity of model at 5% level of significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fissler Ziegel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-642.6625960112602\n",
      "-606.7872701205615\n"
     ]
    }
   ],
   "source": [
    "def fsscore(v,e,x,a):\n",
    "    fs = ((v<=x)-a)*(v-x)+(1/a)*np.exp(e)*(v<=x)*(v-x)+np.exp(e)*(v-x)-np.exp(e)\n",
    "    return fs\n",
    "totfs_ARCH = 0.0\n",
    "totfs_GARCH = 0.0\n",
    "for j in range(Teval):\n",
    "    totfs_ARCH += fsscore(VaR_ARCH[j],ES_ARCH[j],Actual.iloc[j],0.05)\n",
    "    totfs_GARCH += fsscore(VaR_GARCH[j],ES_GARCH[j],Actual.iloc[j],0.05)\n",
    "print(totfs_ARCH)\n",
    "print(totfs_GARCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARCH performs better for ES and VaR forecasts combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-step ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=1000\n",
    "ES_GARCH10=np.zeros(Teval)\n",
    "VaR_GARCH10=np.zeros(Teval)\n",
    "for j in range(Teval):\n",
    "    ret_train = ret[:-Teval+j]\n",
    "    garch = arch_model(ret_train,mean='Constant',vol='GARCH',p=1,q=1)\n",
    "    garchfit=garch.fit(disp='off')\n",
    "    fc = garchfit.forecast(horizon=10,method='simulation',simulations=B)\n",
    "    rtilde = fc.simulations.values[-1,:,9]\n",
    "    var = np.percentile(rtilde,5)\n",
    "    VaR_GARCH10[j]=var\n",
    "    ES_GARCH10[j]=np.mean(rtilde[rtilde<=var])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.2846872555132873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10613216578260404"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi = (Actual-ES_GARCH10)[Actual<VaR_GARCH10]\n",
    "t = np.mean(xi)/(np.std(xi)/np.sqrt(len(xi)))\n",
    "print(t)\n",
    "scipy.stats.t.cdf(t,df = len(xi-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wrap up\n",
    "- Expected Shorfall has many attractive properties relative to Value at Risk\n",
    "- Basel III is currently being implemented by banks, so there is demand for people who know how to estimate it.\n",
    "- The score for comparing different models for ES (the Fissler Ziegel score) is quite new in the literature.\n",
    "- Work remains to be done for backtesting."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "scroll": true,
   "start_slideshow_at": "beginning"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
