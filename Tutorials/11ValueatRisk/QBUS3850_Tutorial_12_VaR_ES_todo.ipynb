{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QBUS3850 Tutorial 12: Forecasting Value at Risk and Expected Shortfall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise uses a progress-bar package called 'tqdm'. The usage is informative but not essential. `pip3 install tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from arch import arch_model\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from tqdm import tqdm, trange\n",
    "from arch.__future__ import reindexing\n",
    "\n",
    "#Set print options to 4dp and suppress scientific notation\n",
    "np.set_printoptions(precision=3, suppress=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import All Ordinaries Index Data\n",
    "AORD_data = pd.read_csv('AllORD0017.csv', parse_dates = ['Date'], dayfirst = True)\n",
    "AOdates = AORD_data['Date']\n",
    "AOp = AORD_data['Close']\n",
    "print( AORD_data.tail() )\n",
    "\n",
    "## Compute log-returns \n",
    "AOr = ( 100 * ( np.log( AOp ) - np.log( AOp.shift() ) ) )[1:]\n",
    "assert( np.isnan( AOr ).sum() == 0 )\n",
    "\n",
    "## Put dates, prices and returns all on the same basis.\n",
    "#AOdates = AOdates[1:]\n",
    "#AOp = AOp[1:]\n",
    "\n",
    "#Plot All Ordinaries index values and returns\n",
    "fig, axs = plt.subplots(2,1, figsize = (10,8))\n",
    "axs[0].plot( AOdates, AOp )\n",
    "axs[0].set_title( 'AllOrds Prices' )\n",
    "axs[1].plot( AOdates[1:], AOr )\n",
    "axs[1].set_title( 'AllOrds log returns' )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 - Forecasting 1 day VaR and ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In-sample and forecast sample\n",
    "n = len(AOr)\n",
    "nis = n-1000\n",
    "AOris = AOr[:nis]\n",
    "AOrf = AOr[nis:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(a) Estimate GARCH(1,1) and GJR-GARCH(1,1) models, with Gaussian and Student-t errors, to the sample of log-returns leaving out the last 1000 days of returns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(b) Forecast 1-step-ahead VaR and ES, updating parameters every 20 days, for each of the last 1000 days of data, using the models in (a). \n",
    "\n",
    "### Q1(c) Include 1-step-ahead VaR and ES forecasts from a 100-day historical simulation. Include the RiskMetrics IGARCH model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VaR_n( p ):\n",
    "    return stats.norm.ppf( p )\n",
    "        \n",
    "def ES_n( p ):\n",
    "    return -stats.norm.pdf( stats.norm.ppf( p ) ) / p\n",
    "\n",
    "def VaR_t( p, df ):\n",
    "    return stats.t.ppf( p, df ) * np.sqrt( ( df - 2 ) / df )\n",
    "\n",
    "def ES_t( p, df ):\n",
    "    return -stats.t.pdf( stats.t.ppf( p, df ), df ) / p * ( df + stats.t.ppf( p, df )**2 ) / ( df - 1 ) * np.sqrt( ( df - 2 ) / df )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1-step-ahead VaR forecasts.\n",
    "VaR1G = []\n",
    "VaR5G = []\n",
    "VaR1Gt = []\n",
    "VaR5Gt = []\n",
    "VaR1GJ = []\n",
    "VaR5GJ = []\n",
    "VaR1GJt = []\n",
    "VaR5GJt = []\n",
    "VaR1RM = []\n",
    "VaR5RM = []\n",
    "VaR1HS = []\n",
    "VaR5HS = []\n",
    "\n",
    "## 1-step-ahead ES forecasts.\n",
    "ES1G = []\n",
    "ES5G = []\n",
    "ES1Gt = []\n",
    "ES5Gt = []\n",
    "ES1GJ = []\n",
    "ES5GJ = []\n",
    "ES1GJt = []\n",
    "ES5GJt = []\n",
    "ES1RM = []\n",
    "ES5RM = []\n",
    "ES1HS = []\n",
    "ES5HS = []\n",
    "\n",
    "## 1-step-ahead volatility forecasts \n",
    "SFAOg = []\n",
    "SFAOgt = []\n",
    "SFAOgj = []\n",
    "SFAOgjt = []\n",
    "SFAOrm = []\n",
    "\n",
    "## Inferred degrees of freedom for t-GARCH models.\n",
    "dfGt = []\n",
    "dfGJt = []\n",
    "\n",
    "for t in trange( nis, n ):\n",
    "    ## Fixed estimation window size\n",
    "    AOris = AOr.values[ t - nis : t ]\n",
    "\n",
    "    ## GARCH models\n",
    "    \n",
    "    # TODO: define the 4 corresponding models   \n",
    "    \n",
    "    if t % 20 == nis % 20: \n",
    "        ## Re-estimate models every 20 observations\n",
    "        EstMdlg = Mdlg.fit(disp='off') \n",
    "        EstMdlgt = Mdlgt.fit(disp='off')\n",
    "        EstMdlgj = Mdlgj.fit(disp='off') \n",
    "        EstMdlgjt = Mdlgjt.fit(disp='off')     \n",
    "    else:  \n",
    "        ## Use updated dataset but previously estimated model parameters.\n",
    "        EstMdlg = Mdlg.fix( params = EstMdlg.params ) \n",
    "        EstMdlgt = Mdlgt.fix( params = EstMdlgt.params ) \n",
    "        EstMdlgj = Mdlgj.fix( params = EstMdlgj.params ) \n",
    "        EstMdlgjt = Mdlgjt.fix( params = EstMdlgjt.params ) \n",
    "        \n",
    "    #1-step-ahead volatility forecasts \n",
    "    SFAOg.append( np.sqrt( EstMdlg.forecast( horizon = 1 ).variance.values[-1, 0] ) )\n",
    "    SFAOgt.append( np.sqrt(EstMdlgt.forecast(horizon = 1).variance.values[-1, 0 ] ) )\n",
    "    SFAOgj.append( np.sqrt(EstMdlgj.forecast(horizon = 1).variance.values[-1, 0 ] ) )\n",
    "    SFAOgjt.append( np.sqrt(EstMdlgjt.forecast(horizon = 1).variance.values[-1, 0 ] ) )\n",
    "    \n",
    "    ## Inferred degrees of freedom\n",
    "    dfGt.append( EstMdlgt.params['nu'] )\n",
    "    dfGJt.append( EstMdlgjt.params['nu'] )\n",
    "    \n",
    "    #1-day ahead VaR forecasts for GARCH-type models\n",
    "    VaR1G.append( EstMdlg.params['mu'] + VaR_n(0.01) * SFAOg[-1] )\n",
    "    VaR5G.append( EstMdlg.params['mu'] + VaR_n(0.05) * SFAOg[-1] )\n",
    "    VaR1Gt.append( EstMdlgt.params['mu'] + VaR_t(0.01, EstMdlgt.params['nu'] ) * SFAOgt[-1] )\n",
    "    VaR5Gt.append( EstMdlgt.params['mu'] + VaR_t(0.05, EstMdlgt.params['nu'] ) * SFAOgt[-1] )\n",
    "    VaR1GJ.append( EstMdlgj.params['mu'] + VaR_n(0.01) * SFAOgj[-1] )\n",
    "    VaR5GJ.append( EstMdlgj.params['mu'] + VaR_n(0.05) * SFAOgj[-1] )\n",
    "    VaR1GJt.append( EstMdlgjt.params['mu'] + VaR_t(0.01, EstMdlgjt.params['nu'] ) * SFAOgjt[-1] )\n",
    "    VaR5GJt.append( EstMdlgjt.params['mu'] + VaR_t(0.05, EstMdlgjt.params['nu'] ) * SFAOgjt[-1] )\n",
    "\n",
    "    #1-day ahead ES forecasts for GARCH-type models\n",
    "    ES1G.append( EstMdlg.params['mu'] + ES_n(0.01) * SFAOg[-1] )\n",
    "    ES5G.append( EstMdlg.params['mu'] + ES_n(0.05) * SFAOg[-1] )\n",
    "    ES1Gt.append( EstMdlgt.params['mu'] + ES_t(0.01, EstMdlgt.params['nu'] ) * SFAOgt[-1] )\n",
    "    ES5Gt.append( EstMdlgt.params['mu'] + ES_t(0.05, EstMdlgt.params['nu'] ) * SFAOgt[-1] )\n",
    "    ES1GJ.append( EstMdlgj.params['mu'] + ES_n(0.01) * SFAOgj[-1] )\n",
    "    ES5GJ.append( EstMdlgj.params['mu'] + ES_n(0.05) * SFAOgj[-1] )\n",
    "    ES1GJt.append( EstMdlgjt.params['mu'] + ES_t(0.01, EstMdlgjt.params['nu'] ) * SFAOgjt[-1] )\n",
    "    ES5GJt.append( EstMdlgjt.params['mu'] + ES_t(0.05, EstMdlgjt.params['nu'] ) * SFAOgjt[-1] )\n",
    "\n",
    "    ## Riskmetrics (IGARCH)\n",
    "    betaRM = 0.94    \n",
    "    if t == nis:\n",
    "        SFAOrm.append( np.std( AOris[-200:] ) )\n",
    "    else:\n",
    "        SFAOrm.append( np.sqrt( ( 1 - betaRM ) * AOris[-1]**2 + betaRM * SFAOrm[-1]**2 ) )\n",
    "\n",
    "    VaR5RM.append( SFAOrm[-1] * VaR_n(0.05) )\n",
    "    VaR1RM.append( SFAOrm[-1] * VaR_n(0.01) )\n",
    "    ES5RM.append( SFAOrm[-1] * ES_n(0.05) )\n",
    "    ES1RM.append( SFAOrm[-1] * ES_n(0.01) )\n",
    "\n",
    "    ## Historical Simulation\n",
    "    VaR5HS.append( np.percentile( AOris[-100:], 5, interpolation = 'midpoint') )\n",
    "    VaR1HS.append( np.percentile( AOris[-100:], 1, interpolation = 'midpoint') )\n",
    "    x100 = AOris[-100:]\n",
    "    ES5HS.append( np.mean( x100[x100<=VaR5HS[-1]] ) )\n",
    "    ES1HS.append( np.mean( x100[x100<=VaR1HS[-1]] ) )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of 5% VaRs \n",
    "fig, axs = plt.subplots(figsize = (10,8))\n",
    "axs.plot(AOdates[-1000:], AOrf, c = 'g', label = 'Returns')\n",
    "axs.plot(AOdates[-1000:], VaR5G, c = 'y', label = 'GARCH')\n",
    "axs.plot(AOdates[-1000:], VaR5Gt, c = 'r', label = 'GARCH-t')\n",
    "axs.plot(AOdates[-1000:], VaR5GJ, c = 'r', label = 'GJR-GARCH')\n",
    "axs.plot(AOdates[-1000:], VaR5GJt, c = 'cyan', label = 'GJR-GARCH-t')\n",
    "axs.plot(AOdates[-1000:], VaR5HS, c = 'b', label = 'HS')\n",
    "axs.plot(AOdates[-1000:], VaR5RM, c = 'k', label = 'RM')\n",
    "axs.set_xlim(AOdates[-1000:].min(), AOdates[-1000:].max())\n",
    "axs.set_title('5% forecast VaRs from all models with actual returns', weight = 'bold')\n",
    "axs.legend()\n",
    "plt.show()\n",
    "\n",
    "#Plot of 1% VaRs \n",
    "fig, axs = plt.subplots(figsize = (10,8))\n",
    "axs.plot(AOdates[-1000:], AOrf, c = 'g', label = 'Returns')\n",
    "axs.plot(AOdates[-1000:], VaR1G, c = 'y', label = 'GARCH')\n",
    "axs.plot(AOdates[-1000:], VaR1Gt, c = 'r', label = 'GARCH-t')\n",
    "axs.plot(AOdates[-1000:], VaR1GJ, c = 'r', label = 'GJR-GARCH')\n",
    "axs.plot(AOdates[-1000:], VaR1GJt, c = 'cyan', label = 'GJR-GARCH-t')\n",
    "axs.plot(AOdates[-1000:], VaR1HS, c = 'b', label = 'HS')\n",
    "axs.plot(AOdates[-1000:], VaR1RM, c = 'k', label = 'RM')\n",
    "axs.set_xlim(AOdates[-1000:].min(), AOdates[-1000:].max())\n",
    "axs.set_title('1% forecast VaRs from all models with actual returns', weight = 'bold')\n",
    "axs.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of 5% ESs \n",
    "fig, axs = plt.subplots(figsize = (10,8))\n",
    "axs.plot(AOdates[-1000:], AOrf, c = 'g', label = 'Returns')\n",
    "axs.plot(AOdates[-1000:], ES5G, c = 'y', label = 'GARCH')\n",
    "axs.plot(AOdates[-1000:], ES5Gt, c = 'r', label = 'GARCH-t')\n",
    "axs.plot(AOdates[-1000:], ES5GJ, c = 'r', label = 'GJR-GARCH')\n",
    "axs.plot(AOdates[-1000:], ES5GJt, c = 'cyan', label = 'GJR-GARCH-t')\n",
    "axs.plot(AOdates[-1000:], ES5HS, c = 'b', label = 'HS')\n",
    "axs.plot(AOdates[-1000:], ES5RM, c = 'k', label = 'RM')\n",
    "axs.set_xlim(AOdates[-1000:].min(), AOdates[-1000:].max())\n",
    "axs.set_title('5% forecast ES from all models with actual returns', weight = 'bold')\n",
    "axs.legend()\n",
    "plt.show()\n",
    "\n",
    "#Plot of 1% ESs \n",
    "fig, axs = plt.subplots(figsize = (10,8))\n",
    "axs.plot(AOdates[-1000:], AOrf, c = 'g', label = 'Returns')\n",
    "axs.plot(AOdates[-1000:], ES1G, c = 'y', label = 'GARCH')\n",
    "axs.plot(AOdates[-1000:], ES1Gt, c = 'r', label = 'GARCH-t')\n",
    "axs.plot(AOdates[-1000:], ES1GJ, c = 'r', label = 'GJR-GARCH')\n",
    "axs.plot(AOdates[-1000:], ES1GJt, c = 'cyan', label = 'GJR-GARCH-t')\n",
    "axs.plot(AOdates[-1000:], ES1HS, c = 'b', label = 'HS')\n",
    "axs.plot(AOdates[-1000:], ES1RM, c = 'k', label = 'RM')\n",
    "axs.set_xlim(AOdates[-1000:].min(), AOdates[-1000:].max())\n",
    "axs.set_title('1% forecast ES from all models with actual returns', weight = 'bold')\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(d) Assess VaR violations for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% CI for violation rates\n",
    "print([0.05 - 1.96 * np.sqrt(0.05 * 0.95 / 1000), 0.05 + 1.96 * np.sqrt(0.05 * 0.95 / 1000) ] )\n",
    "print([0.01 - 1.96 * np.sqrt(0.01 * 0.99 / 1000), 0.01 + 1.96 * np.sqrt(0.01 * 0.99 / 1000) ] )\n",
    "\n",
    "#accuracy of violation rates\n",
    "# TODO: assess VaR violation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HS method is closest to the nominal violation rate at both 5% and 1%! Among GARCH-type models, RiskMetrics is closest to the 5% rate and GARCH-t to the 1% rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol5G = ( AOrf<VaR5G )\n",
    "viol5GJ = ( AOrf<VaR5GJ )\n",
    "viol5Gt = ( AOrf<VaR5Gt )\n",
    "viol5GJt = ( AOrf<VaR5GJt )\n",
    "viol5RM = ( AOrf<VaR5RM )\n",
    "viol5HS = ( AOrf<VaR5HS )\n",
    "\n",
    "## p = 0.05 ES residuals (magnitudes of VaR violations)\n",
    "err5G = ( AOrf - ES5G )[ viol5G ]\n",
    "err5GJ = ( AOrf - ES5GJ )[ viol5GJ ]\n",
    "err5Gt = ( AOrf - ES5Gt )[ viol5Gt]\n",
    "err5GJt = ( AOrf - ES5GJt )[ viol5GJt ]\n",
    "err5RM = ( AOrf - ES5RM )[ viol5RM ]\n",
    "err5HS = ( AOrf - ES5HS )[ viol5HS]\n",
    "\n",
    "#Plots of ES residuals for p=0.05\n",
    "fig, axs = plt.subplots(figsize = (10,8))\n",
    "axs.plot(err5G.values, c = 'y', label = 'GARCH')\n",
    "axs.plot(err5Gt.values, c = 'm', label = 'GARCH-t')\n",
    "axs.plot(err5GJ.values, c = 'r', label = 'GJR GARCH')\n",
    "axs.plot(err5GJt.values, c = 'c', label = 'GJR GARCH-t')\n",
    "axs.plot(err5HS.values, c = 'b', label = 'HS')\n",
    "axs.plot(err5RM.values, c = 'k', label = 'RM')\n",
    "axs.set_title('ES Residuals from all models at p=0.05', weight = 'bold')\n",
    "axs.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viol1G = ( AOrf<VaR1G )\n",
    "viol1GJ = ( AOrf<VaR1GJ )\n",
    "viol1Gt = ( AOrf<VaR1Gt )\n",
    "viol1GJt = ( AOrf<VaR1GJt )\n",
    "viol1RM = ( AOrf<VaR1RM )\n",
    "viol1HS = ( AOrf<VaR1HS )\n",
    "\n",
    "## p = 0.01 ES residuals (magnitudes of VaR violations)\n",
    "err1G = ( AOrf - ES1G )[ viol1G ]\n",
    "err1GJ = ( AOrf - ES1GJ )[ viol1GJ ]\n",
    "err1Gt = ( AOrf - ES1Gt )[ viol1Gt]\n",
    "err1GJt = ( AOrf - ES1GJt )[ viol1GJt ]\n",
    "err1RM = ( AOrf - ES1RM )[ viol1RM ]\n",
    "err1HS = ( AOrf - ES1HS )[ viol1HS]\n",
    "\n",
    "#Plots of ES residuals for p=0.01\n",
    "fig, axs = plt.subplots(figsize = (10,8))\n",
    "axs.plot(err1G.values, c = 'y', label = 'GARCH')\n",
    "axs.plot(err1Gt.values, c = 'm', label = 'GARCH-t')\n",
    "axs.plot(err1GJ.values, c = 'r', label = 'GJR GARCH')\n",
    "axs.plot(err1GJt.values, c = 'c', label = 'GJR GARCH-t')\n",
    "axs.plot(err1HS.values, c = 'b', label = 'HS')\n",
    "axs.plot(err1RM.values, c = 'k', label = 'RM')\n",
    "axs.set_title('ES Residuals from all models at p=0.01', weight = 'bold')\n",
    "axs.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## p = 0.05 standardised ES residuals (scaled by volatility)\n",
    "err5Gs = ( ( AOrf - ES5G ) / SFAOg )[viol5G]\n",
    "err5GJs = ( ( AOrf - ES5GJ ) / SFAOgj )[viol5GJ]\n",
    "err5Gts = ( ( AOrf - ES5Gt ) / SFAOgt )[viol5Gt]\n",
    "err5GJts = ( ( AOrf - ES5GJt ) / SFAOgjt )[viol5GJt]\n",
    "err5RMs = ( ( AOrf - ES5RM ) / SFAOrm )[viol5RM]\n",
    "err5HSs = ( ( AOrf - ES5HS ) / SFAOrm )[viol5HS]\n",
    "\n",
    "#Plots of ES residuals for p=0.05\n",
    "fig, axs = plt.subplots(figsize = (10,8))\n",
    "axs.plot(err5Gs.values, c = 'y', label = 'GARCH')\n",
    "axs.plot(err5Gts.values, c = 'm', label = 'GARCH-t')\n",
    "axs.plot(err5GJs.values, c = 'r', label = 'GJR GARCH')\n",
    "axs.plot(err5GJts.values, c = 'c', label = 'GJR GARCH-t')\n",
    "axs.plot(err5HSs.values, c = 'b', label = 'HS')\n",
    "axs.plot(err5RMs.values, c = 'k', label = 'RM')\n",
    "axs.set_title('Standardised ES Residuals from all models at p=0.05' )\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## p = 0.01 standardised ES residuals (scaled by volatility)\n",
    "err1Gs = ( ( AOrf - ES1G ) / SFAOg )[viol1G]\n",
    "err1GJs = ( ( AOrf - ES1GJ ) / SFAOgj )[viol1GJ]\n",
    "err1Gts = ( ( AOrf - ES1Gt ) / SFAOgt )[viol1Gt]\n",
    "err1GJts = ( ( AOrf - ES1GJt ) / SFAOgjt )[viol1GJt]\n",
    "err1RMs = ( ( AOrf - ES1RM ) / SFAOrm )[viol1RM]\n",
    "err1HSs = ( ( AOrf - ES1HS ) / SFAOrm )[viol1HS]\n",
    "\n",
    "#Plots of ES residuals for p=0.01\n",
    "fig, axs = plt.subplots(figsize = (10,8))\n",
    "axs.plot(err1Gs.values, c = 'y', label = 'GARCH')\n",
    "axs.plot(err1Gts.values, c = 'm', label = 'GARCH-t')\n",
    "axs.plot(err1GJs.values, c = 'r', label = 'GJR GARCH')\n",
    "axs.plot(err1GJts.values, c = 'c', label = 'GJR GARCH-t')\n",
    "axs.plot(err1HSs.values, c = 'b', label = 'HS')\n",
    "axs.plot(err1RMs.values, c = 'k', label = 'RM')\n",
    "axs.set_title('Standardised ES Residuals from all models at p=0.01' )\n",
    "axs.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For p=0.05: sums, means and t-stats for ES residuals as well as means and t-stats for standardised ES residuals\n",
    "print('\\n', np.array([[np.sum(viol5G), np.sum(viol5GJ), np.sum(viol5RM), np.sum(viol5Gt), np.sum(viol5GJt), np.sum(viol5HS)], \n",
    "               [np.mean(err5G), np.mean(err5GJ), np.mean(err5RM), np.mean(err5Gt), np.mean(err5GJt), np.mean(err5HS)], \n",
    "               [np.mean(err5G)/(np.std(err5G)/np.sqrt(len(err5G))), np.mean(err5GJ)/(np.std(err5GJ)/np.sqrt(len(err5GJ))), np.mean(err5RM)/(np.std(err5RM)/np.sqrt(len(err5RM))), np.mean(err5Gt)/(np.std(err5Gt)/np.sqrt(len(err5Gt))), np.mean(err5GJt)/(np.std(err5GJt)/np.sqrt(len(err5GJt))), np.mean(err5HS)/(np.std(err5HS)/np.sqrt(len(err5HS)))], \n",
    "               [np.mean(err5Gs), np.mean(err5GJs), np.mean(err5RMs), np.mean(err5Gts), np.mean(err5GJts), np.mean(err5HSs)], \n",
    "               [np.mean(err5Gs)/(np.std(err5Gs)/np.sqrt(len(err5Gs))), np.mean(err5GJs)/(np.std(err5GJs)/np.sqrt(len(err5GJs))), np.mean(err5RMs)/(np.std(err5RMs)/np.sqrt(len(err5RMs))), np.mean(err5Gts)/(np.std(err5Gts)/np.sqrt(len(err5Gts))), np.mean(err5GJts)/(np.std(err5GJts)/np.sqrt(len(err5GJts))), np.mean(err5HSs)/(np.std(err5HSs)/np.sqrt(len(err5HSs)))]]))\n",
    "\n",
    "#For p=0.01: sums, means and t-stats for ES residuals as well as means and t-stats for standardised ES residuals\n",
    "print('\\n', np.array([[np.sum(viol1G), np.sum(viol1GJ), np.sum(viol1RM), np.sum(viol1Gt), np.sum(viol1GJt), np.sum(viol1HS)], \n",
    "               [np.mean(err1G), np.mean(err1GJ), np.mean(err1RM), np.mean(err1Gt), np.mean(err1GJt), np.mean(err1HS)], \n",
    "               [np.mean(err1G)/(np.std(err1G)/np.sqrt(len(err1G))), np.mean(err1GJ)/(np.std(err1GJ)/np.sqrt(len(err1GJ))), np.mean(err1RM)/(np.std(err1RM)/np.sqrt(len(err1RM))), np.mean(err1Gt)/(np.std(err1Gt)/np.sqrt(len(err1Gt))), np.mean(err1GJt)/(np.std(err1GJt)/np.sqrt(len(err1GJt))), np.mean(err1HS)/(np.std(err1HS)/np.sqrt(len(err1HS)))], \n",
    "               [np.mean(err1Gs), np.mean(err1GJs), np.mean(err1RMs), np.mean(err1Gts), np.mean(err1GJts), np.mean(err1HSs)], \n",
    "               [np.mean(err1Gs)/(np.std(err1Gs)/np.sqrt(len(err1Gs))), np.mean(err1GJs)/(np.std(err1GJs)/np.sqrt(len(err1GJs))), np.mean(err1RMs)/(np.std(err1RMs)/np.sqrt(len(err1RMs))), np.mean(err1Gts)/(np.std(err1Gts)/np.sqrt(len(err1Gts))), np.mean(err1GJts)/(np.std(err1GJts)/np.sqrt(len(err1GJts))), np.mean(err1HSs)/(np.std(err1HSs)/np.sqrt(len(err1HSs)))]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the GARCH-t, GJR-t models have average ES residuals not significantly different from 0, at both 5% and 1% levels. The GARCH and HS100 models can be rejected for having average 1% ES residuals significantly different from 0. Both models have significantly negative residuals, meaning that their ES forecasts are not extreme enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE and MAD\n",
    "print( 'G', 'GJ', 'RM', 'G-t', 'GJ-t', 'HS')\n",
    "print( np.array([[np.sqrt(np.mean(err5G**2)), np.sqrt(np.mean(err5GJ**2)), np.sqrt(np.mean(err5RM**2)), np.sqrt(np.mean(err5Gt**2)), np.sqrt(np.mean(err5GJt**2)), np.sqrt(np.mean(err5HS**2))], \n",
    "          [np.sqrt(np.mean(err1G**2)), np.sqrt(np.mean(err1GJ**2)), np.sqrt(np.mean(err1RM**2)), np.sqrt(np.mean(err1Gt**2)), np.sqrt(np.mean(err1GJt**2)), np.sqrt(np.mean(err1HS**2))], \n",
    "          [np.mean(np.abs(err5G)), np.mean(np.abs(err5GJ)), np.mean(np.abs(err5RM)), np.mean(np.abs(err5Gt)), np.mean(np.abs(err5GJt)), np.mean(np.abs(err5HS))], \n",
    "          [np.mean(np.abs(err1G)), np.mean(np.abs(err1GJ)), np.mean(np.abs(err1RM)), np.mean(np.abs(err1Gt)), np.mean(np.abs(err1GJt)), np.mean(np.abs(err1HS))]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE and MAD are suitable loss functions for forecasting ES accurately. Overall the GARCH-t model appears best under these criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the Gaussian error models:\n",
    "##  - the ES 5% occurs at the 0.0196 (or 1.96%) quantile,\n",
    "##  - the ES 1% occurs at the 0.0038 (or 0.38%) quantile.\n",
    "delaN5 = stats.norm.cdf( ES_n( 0.05 ) )\n",
    "delaN1 = stats.norm.cdf( ES_n( 0.01 ) )\n",
    "\n",
    "## For the Student-t error models, use the average degree of freedom from G-t and GJR-t \n",
    "## to determine what quantile the 5% and 1% ES should occur at, for that Student-t:\n",
    "dfGt_avg = np.mean(dfGt)\n",
    "dfGJt_avg = np.mean(dfGJt)\n",
    "delaG5 = stats.t.cdf( ES_t( 0.05, dfGt_avg ), dfGt_avg )\n",
    "delaG1 = stats.t.cdf( ES_t( 0.01, dfGt_avg ), dfGt_avg )\n",
    "delaGJ5 = stats.t.cdf( ES_t( 0.05, dfGJt_avg ), dfGJt_avg )\n",
    "delaGJ1 = stats.t.cdf( ES_t( 0.01, dfGJt_avg ), dfGJt_avg )\n",
    "\n",
    "print( ' G      ', 'GJ     ', 'RM     ', 'G-t    ', 'GJ-t   ', 'HS     ' )\n",
    "#Set print options to 4dp and suppress scientific notation\n",
    "np.set_printoptions(precision=5, suppress=True) \n",
    "print( np.array( [delaN5, delaN5, delaN5, delaG5, delaGJ5, (delaG5+delaGJ5)/2] ) ) \n",
    "print( np.array( [delaN1, delaN1, delaN1, delaG1, delaGJ1, (delaG1+delaGJ1)/2] ) )\n",
    "\n",
    "#dfGt = np.array(dfGt)\n",
    "#dfGJt = np.array(dfGJt)\n",
    "#delaG5t = stats.t.cdf( ES_t( 0.05, dfGt ), dfGt )\n",
    "#delaG1t = stats.t.cdf( ES_t( 0.01, dfGt ), dfGt )\n",
    "#delaGJ5t = stats.t.cdf( ES_t( 0.05, dfGJt ), dfGJt )\n",
    "#delaGJ1t = stats.t.cdf( ES_t( 0.01, dfGJt ), dfGJt )\n",
    "\n",
    "### Plot Changes in 1% ES quantile estimate\n",
    "#fig, axs = plt.subplots(figsize = (10,8))\n",
    "#axs.plot(AOdates[-1000:], delaG1t, c = 'b', label = 'GARCH-t')\n",
    "#axs.plot(AOdates[-1000:], delaGJ1t, c = 'r', label = 'GJR-GARCH-t')\n",
    "#axs.set_title('Changes in 1% ES quantile estimate for GARCH-t and GJR-t', weight = 'bold')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ES 5% and ES 1% occur at higher quantiles for the Student-t than for the Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Violations of ES forecasts\n",
    "viol5VG = (AOrf<ES5G)\n",
    "viol5VGJ = (AOrf<ES5GJ)\n",
    "viol5VRM =(AOrf<ES5RM)\n",
    "viol5VGt = (AOrf<ES5Gt)\n",
    "viol5VGJt = (AOrf<ES5GJt)\n",
    "viol5VHS = (AOrf<ES5HS)\n",
    "viol1VG = (AOrf<ES1G)\n",
    "viol1VGJ = (AOrf<ES1GJ)\n",
    "viol1VRM = (AOrf<ES1RM)\n",
    "viol1VGt = (AOrf<ES1Gt)\n",
    "viol1VGJt = (AOrf<ES1GJt)\n",
    "viol1VHS = (AOrf<ES1HS)\n",
    "\n",
    "#Binomial confidence intervals for Gaussian and t models at 5% and 1% \n",
    "def binomial_ci( p ):\n",
    "    return np.array( [ p - 1.96*np.sqrt( p*(1-p)/1000 ), p + 1.96*np.sqrt( p*(1-p)/1000 ) ] )\n",
    "\n",
    "print('Gaussian 5% ', binomial_ci( delaN5 ) )\n",
    "print('Student-t 5%', binomial_ci( delaG5 ) )\n",
    "print('Gaussian 1% ', binomial_ci( delaN1 ) )\n",
    "print('Student-t 1%', binomial_ci( delaG1 ) )\n",
    "\n",
    "## Expect ES 5% violation rates for the Gaussian models in the range (0.0110, 0.0282) \n",
    "## with 95% confidence.\n",
    "## Expect higher ES 5% violation rates for the Student-t models, in the range (0.0178, 0.0382)\n",
    "## with 95% confidence.\n",
    "\n",
    "## ES 5% violations \n",
    "print( [np.sum(viol5VG), np.sum(viol5VGJ), np.sum(viol5VRM), np.sum(viol5VGt), np.sum(viol5VGJt), np.sum(viol5VHS)] )\n",
    "\n",
    "## ES 5% violation rates\n",
    "print( [np.sum(viol5VG)/1000, np.sum(viol5VGJ)/1000, np.sum(viol5VRM)/1000, np.sum(viol5VGt)/1000, np.sum(viol5VGJt)/1000, np.sum(viol5VHS)/1000] )\n",
    "\n",
    "## ES 1% violations \n",
    "print( [np.sum(viol1VG), np.sum(viol1VGJ), np.sum(viol1VRM), np.sum(viol1VGt), np.sum(viol1VGJt), np.sum(viol1VHS)] )\n",
    "\n",
    "## ES 1% violation rates\n",
    "print( [np.sum(viol1VG)/1000, np.sum(viol1VGJ)/1000, np.sum(viol1VRM)/1000, np.sum(viol1VGt)/1000, np.sum(viol1VGJt)/1000, np.sum(viol1VHS)/1000] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian models have too many violations, both at 5% and 1%, and can be rejected as under-estimating 5% ES risk levels. The Student-t models agree well with expected violation rates, both at 5% and 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1(e) Assess the models for ES forecast accuracy, independence and loss function values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the 3 functions introduced in tutorial 11.\n",
    "def indtest(hits):\n",
    "    n = len(hits)\n",
    "\n",
    "    r5 = hits[1:]\n",
    "    r51 = hits[:-1]\n",
    "    i11 = r5*r51\n",
    "    i01 = r5*(1-r51)\n",
    "    i10 = (1-r5)*r51\n",
    "    i00 = (1-r5)*(1-r51)\n",
    "\n",
    "    t00 = np.sum(i00)\n",
    "    t01 = np.sum(i01)\n",
    "    t10 = np.sum(i10)\n",
    "    t11 = np.sum(i11)\n",
    "    p01 = t01/(t00+t01)\n",
    "    p11 = t11/(t10+t11)\n",
    "    p1 = (t01+t11)/n\n",
    "\n",
    "    ll1 = t00 * np.log(1-p01) + (p01>0) * t01 * np.log(p01) + t10 * np.log(1-p11)\n",
    "    if p11>0:\n",
    "        ll1=ll1+t11*np.log(p11)\n",
    "  \n",
    "    ll0=(t10+t00)*np.log(1-p1)+(t01+t11)*np.log(p1)\n",
    "\n",
    "    lrind=2*(ll1-ll0)\n",
    "    pcc=1-stats.chi2.cdf(lrind,1)\n",
    "    return pcc, lrind\n",
    "\n",
    "def dqtest(y,f,a,lag):\n",
    "    n = len(y)\n",
    "    hits = ((y<f)*1)*(1-a)\n",
    "    hits = (hits)*1+(y>f)*(-a)\n",
    "    q=2+lag\n",
    "    \n",
    "    if np.sum((y<f)*1) > 0:\n",
    "        ns = n - lag\n",
    "        xmat = np.column_stack([np.ones((ns,1)), f[lag:n+1]])\n",
    "        for k in range(1,lag+1):\n",
    "            lk = lag-k\n",
    "            xmat = np.column_stack([xmat, hits[lk:n-k]])\n",
    "    \n",
    "        hx = np.dot((hits[lag:n+1]), xmat)\n",
    "        xtx = np.linalg.lstsq(np.matmul(xmat.T, xmat), np.eye(q), rcond = None)[0]\n",
    "        dq = np.dot(np.dot(hx, xtx), hx.T)\n",
    "        dq = dq/(a*(1-a))\n",
    "        pdq = 1 - stats.chi2.cdf(dq,q)\n",
    "    else:\n",
    "        pdq = np.nan\n",
    "        dq = np.nan\n",
    "    return pdq, dq\n",
    "\n",
    "def qregloss(q,r,p):\n",
    "    q = np.array( q )\n",
    "    x1 = r[r > q]\n",
    "    x2 = r[r < q]\n",
    "    f1 = q[r > q]\n",
    "    f2 = q[r < q]\n",
    "    qrgl = p * np.sum(x1-f1) + (1-p) * np.sum(f2-x2)\n",
    "    return qrgl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5% Independence tests\n",
    "pG,lrG = indtest(viol5VG.values)\n",
    "pGJ,lrGJ = indtest(viol5VGJ.values)\n",
    "pRM,lrRM = indtest(viol5VRM.values)\n",
    "pGt,lrGt = indtest(viol5VGt.values)\n",
    "pGJt,lrGJt = indtest(viol5VGJt.values)\n",
    "pHS1,lrHS1 = indtest(viol5VHS.values)\n",
    "print('5% Independence tests', [round(x,4) for x in [pG, pGJ, pRM, pGt, pGJt, pHS1]])\n",
    "\n",
    "\n",
    "dela5 = [delaN5, delaN5, delaN5, delaG5, delaGJ5, (delaG5+delaGJ5)/2]\n",
    "dela1 = [delaN1, delaN1, delaN1, delaG1, delaGJ1, (delaG1+delaGJ1)/2]\n",
    "\n",
    "#5% DQ tests\n",
    "pdqG,dqG = dqtest(AOrf,ES5G,0.0196,4)\n",
    "pdqGJ,dqGJ = dqtest(AOrf,ES5GJ,0.0196,4);\n",
    "pdqRM,dqRM = dqtest(AOrf,ES5RM,0.0196,4);\n",
    "pdqGt,dqGt = dqtest(AOrf,ES5Gt,delaG5,4);\n",
    "pdqGJt,dqGJt = dqtest(AOrf,ES5GJt,delaGJ5,4);\n",
    "pdqHS,dqHS =dqtest(AOrf,ES5HS,dela5[5],4);\n",
    "print('5% DQ tests', [round(x, 4) for x in [pdqG, pdqGJ, pdqRM, pdqGt, pdqGJt, pdqHS]])\n",
    "\n",
    "    #1% Independence tests\n",
    "pG,lrG = indtest(viol1VG.values)\n",
    "pGJ,lrGJ = indtest(viol1VGJ.values)\n",
    "pRM,lrRM = indtest(viol1VRM.values)\n",
    "pGt,lrGt = indtest(viol1VGt.values)\n",
    "pGJt,lrGJt = indtest(viol1VGJt.values)\n",
    "pHS1,lrHS1 = indtest(viol1VHS.values)\n",
    "print('1% Independence tests', [round(x,4) for x in [pG, pGJ, pRM, pGt, pGJt, pHS1]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#1% DQ tests\n",
    "pdqG,dqG = dqtest(AOrf,ES1G,0.0038,4)\n",
    "pdqGJ,dqGJ = dqtest(AOrf,ES1GJ,0.0038,4);\n",
    "pdqRM,dqRM = dqtest(AOrf,ES1RM,0.0038,4);\n",
    "pdqGt,dqGt = dqtest(AOrf,ES1Gt,delaG1,4);\n",
    "pdqGJt,dqGJt = dqtest(AOrf,ES1GJt,delaGJ1,4);\n",
    "pdqHS,dqHS =dqtest(AOrf,ES1HS,dela1[5],4);\n",
    "print('1% DQ tests', [round(x, 4) for x in [pdqG, pdqGJ, pdqRM, pdqGt, pdqGJt, pdqHS]])\n",
    "\n",
    "#5% Quantile Loss functions\n",
    "loss5G = qregloss(ES5G,AOrf,0.0196)\n",
    "loss5GJ = qregloss(ES5GJ,AOrf,0.0196)\n",
    "loss5RM = qregloss(ES5RM,AOrf,0.0196)\n",
    "loss5Gt = qregloss(ES5Gt,AOrf,delaG5);\n",
    "loss5GJt = qregloss(ES5GJt,AOrf,delaGJ5)\n",
    "loss5HS = qregloss(ES5HS,AOrf, (delaG5 + delaGJ5)/2 )\n",
    "print('5% Loss Functions', [round(x, 4) for x in [loss5G, loss5GJ, loss5RM, loss5Gt, loss5GJt, loss5HS]])\n",
    "\n",
    "#1% Quantile Loss functions\n",
    "loss1G = qregloss(ES1G,AOrf ,0.0038)\n",
    "loss1GJ = qregloss(ES1GJ,AOrf ,0.0038)\n",
    "loss1RM = qregloss(ES1RM,AOrf ,0.0038)\n",
    "loss1Gt = qregloss(ES1Gt,AOrf ,delaG1);\n",
    "loss1GJt = qregloss(ES1GJt,AOrf ,delaGJ1)\n",
    "loss1HS = qregloss(ES1HS,AOrf ,dela1[5])\n",
    "print('1% Loss Functions', [round(x, 4) for x in [loss1G, loss1GJ, loss1RM, loss1Gt, loss1GJt, loss1HS]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ES forecasts cannot be rejected for any model by the independence test. However, the Gaussian models are rejected by the more powerful DQ test at both 5% and 1%, while the Student-t and HS models are not rejected.\n",
    "\n",
    "Unexpectedly, the quantile loss function favours the Gaussian models over the Student-t models.\n",
    "I am not sure why this is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 VaR and ES forecasting, h > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider the All Ordinaries Index data from Q1, now using the full sample.\n",
    "\n",
    "### Q2(a) Using the models from Q1, forecast the VaR and ES at p = 0.05, 0.01, for the last 10-day period in the sample. Did the actual 10-day return fit in with the forecast 10-day return distributions, VaR and ES measures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed( 0x123123 )\n",
    "\n",
    "#Q2(a) Forecast 10-day VaR and ES\n",
    "    #Last 10 days of sample\n",
    "nc = len(AOr)\n",
    "ncis = nc-10\n",
    "AOris = AOr[:ncis]\n",
    "\n",
    "#Realised 10-day return over last 10 days\n",
    "AO10 = np.sum(AOr[-10:])\n",
    "\n",
    "#Define and estimate models\n",
    "Mdlg = arch_model(y = AOris, mean = 'constant', vol = 'GARCH', p = 1, q = 1, dist = 'gaussian')\n",
    "Mdlgt = arch_model(y = AOris, mean = 'constant', vol = 'GARCH', p = 1, q = 1, dist = 'studentst')\n",
    "Mdlgj = arch_model(y = AOris, mean = 'constant', vol = 'GARCH', p = 1, q = 1, o = 1, dist = 'gaussian')\n",
    "Mdlgjt = arch_model(y = AOris, mean = 'constant', vol = 'GARCH', p = 1, q = 1, o = 1, dist = 'studentst')\n",
    "\n",
    "EstMdlg = Mdlg.fit( disp='off' )\n",
    "EstMdlgt = Mdlgt.fit( disp='off' )\n",
    "EstMdlgj = Mdlgj.fit( disp='off' )\n",
    "EstMdlgjt = Mdlgjt.fit( disp='off' )\n",
    "\n",
    "\n",
    "## Simulate 10-day returns.\n",
    "def step_10_g():\n",
    "    a1G = EstMdlg.params['alpha[1]']\n",
    "    a0G = EstMdlg.params['omega']\n",
    "    b1G = EstMdlg.params['beta[1]']\n",
    "    p0G = EstMdlg.params['mu']\n",
    "\n",
    "    sret = np.zeros( ( nsim ) )\n",
    "    sf = np.zeros( ( nsim ) )\n",
    "    \n",
    "    for h in range( 0, 10 ):\n",
    "        if h == 0:\n",
    "            sf.fill( np.sqrt( EstMdlg.forecast( horizon = 1 ).variance.values[-1] )[0] )\n",
    "        else:\n",
    "            sf = np.sqrt( a0G + a1G * a**2 + b1G * sf**2 ) \n",
    "          \n",
    "        e = np.random.normal( size = nsim )\n",
    "        a = sf * e \n",
    "        sret += p0G + a\n",
    "    return sret\n",
    "\n",
    "def step_10_gt():\n",
    "    a1Gt = EstMdlgt.params['alpha[1]']\n",
    "    a0Gt = EstMdlgt.params['omega']\n",
    "    b1Gt = EstMdlgt.params['beta[1]']\n",
    "    p0Gt = EstMdlgt.params['mu']\n",
    "    dfGt = EstMdlgt.params['nu']\n",
    "\n",
    "    sret = np.zeros( ( nsim ) )\n",
    "    sf = np.zeros( ( nsim ) )\n",
    "\n",
    "    for h in range( 0, 10 ):\n",
    "        if h == 0:\n",
    "            sf.fill( np.sqrt( EstMdlgt.forecast( horizon = 1 ).variance.values[-1] )[0] )\n",
    "        else:\n",
    "            sf = np.sqrt( a0Gt + a1Gt * a**2 + b1Gt * sf**2 ) \n",
    "\n",
    "        e = np.random.standard_t( df = dfGt, size = nsim ) * np.sqrt( ( dfGt - 2 ) / dfGt )\n",
    "        a = sf * e \n",
    "        sret += p0Gt + a\n",
    "    return sret\n",
    "    \n",
    "def step_10_gj():    \n",
    "    a1GJ = EstMdlgj.params['alpha[1]']\n",
    "    a0GJ = EstMdlgj.params['omega']\n",
    "    b1GJ = EstMdlgj.params['beta[1]']\n",
    "    p0GJ = EstMdlgj.params['mu']\n",
    "    g1GJ = EstMdlgj.params['gamma[1]']\n",
    "    \n",
    "    sret = np.zeros( ( nsim ) )\n",
    "    sf = np.zeros( ( nsim ) )\n",
    "\n",
    "    for h in range( 0, 10 ):\n",
    "        if h == 0:\n",
    "            sf.fill( np.sqrt( EstMdlgj.forecast( horizon = 1 ).variance.values[-1] )[0] )\n",
    "        else:\n",
    "            sf = np.sqrt( a0GJ + ( a1GJ + g1GJ * ( a < 0 ) ) * a**2 + b1GJ * sf**2 ) \n",
    "           \n",
    "        e = np.random.normal( size = nsim )\n",
    "        a = sf * e \n",
    "        sret += p0GJ + a\n",
    "    return sret\n",
    "    \n",
    "def step_10_gjt():    \n",
    "    a1GJt = EstMdlgjt.params['alpha[1]']\n",
    "    a0GJt = EstMdlgjt.params['omega']\n",
    "    b1GJt = EstMdlgjt.params['beta[1]']\n",
    "    p0GJt = EstMdlgjt.params['mu']\n",
    "    g1GJt = EstMdlgjt.params['gamma[1]']\n",
    "    dfGJt = EstMdlgjt.params['nu'] \n",
    "\n",
    "    sret = np.zeros( ( nsim ) )\n",
    "    sf = np.zeros( ( nsim ) )\n",
    "    \n",
    "    for h in range( 0, 10 ):\n",
    "        if h == 0:\n",
    "            sf.fill( np.sqrt( EstMdlgt.forecast( horizon = 1 ).variance.values[-1] )[0] )\n",
    "        else:\n",
    "            sf = np.sqrt( a0GJt + ( a1GJt + g1GJt * ( a < 0 ) ) * a**2 + b1GJt * sf**2 ) \n",
    "          \n",
    "        e = np.random.standard_t( df = dfGJt, size = nsim ) * np.sqrt( ( dfGJt - 2 ) / dfGJt )\n",
    "        a = sf * e \n",
    "        sret += p0GJt + a\n",
    "    return sret\n",
    " \n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "nsim = 100000\n",
    "    \n",
    "sretg = step_10_g()\n",
    "sretgt = step_10_gt()\n",
    "sretgj = step_10_gj()    \n",
    "sretgjt = step_10_gjt()\n",
    "\n",
    "#Calculate VaRs at 5% and 1% level from nsim simulated 10-day returns\n",
    "VaR5G10 = np.percentile(sretg, q = 5, interpolation = 'midpoint')\n",
    "VaR1G10 = np.percentile(sretg, q = 1, interpolation = 'midpoint')\n",
    "VaR5GJ10 = np.percentile(sretgj, q = 5, interpolation = 'midpoint')\n",
    "VaR1GJ10 = np.percentile(sretgj, q = 1, interpolation = 'midpoint')\n",
    "VaR5Gt10 = np.percentile(sretgt, q = 5, interpolation = 'midpoint')\n",
    "VaR1Gt10 = np.percentile(sretgt, q = 1, interpolation = 'midpoint')\n",
    "VaR5GJt10 = np.percentile(sretgjt, q = 5, interpolation = 'midpoint')\n",
    "VaR1GJt10 = np.percentile(sretgjt, q = 1, interpolation = 'midpoint')\n",
    "print('\\nVaRs', [round(x,4) for x in [VaR5G10, VaR5GJ10, VaR5Gt10, VaR5GJt10, VaR1G10, VaR1GJ10, VaR1Gt10, VaR1GJt10]])\n",
    "\n",
    "#Calculate ES at 5% and 1% level from nsim simulated 10-day returns\n",
    "ES5G10 = np.mean(sretg[sretg<=VaR5G10])\n",
    "ES1G10 = np.mean(sretg[sretg<=VaR1G10])\n",
    "ES5GJ10 = np.mean(sretgj[sretgj<=VaR5GJ10])\n",
    "ES1GJ10 = np.mean(sretgj[sretgj<=VaR1GJ10])\n",
    "ES5Gt10 = np.mean(sretgt[sretgt<=VaR5Gt10])\n",
    "ES1Gt10 = np.mean(sretgt[sretgt<=VaR1Gt10])\n",
    "ES5GJt10 = np.mean(sretgjt[sretgjt<=VaR5GJt10])\n",
    "ES1GJt10 = np.mean(sretgjt[sretgjt<=VaR1GJt10])\n",
    "print('\\nExpected Shortfall', [round(x,4) for x in [ES5G10, ES5GJ10, ES5Gt10, ES5GJt10, ES1G10, ES1GJ10, ES1Gt10, ES1GJt10]])\n",
    "\n",
    "print(\"Elapsed: %fs.\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(b) Plot the 10-day returns distributions from (a) and comment. Test the number of Monte Carlo samples needed to get reliable and repeatable 10-day VaR and ES forecasts. How much time does each run take at this MC sample size? Is that a reasonable or practical amount of time for this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograms of 10-day returns distributions\n",
    "plt.hist(sretg, bins = 50)\n",
    "plt.title('Histogram of simulated 10-day return distribution - GARCH')\n",
    "plt.show()\n",
    "plt.hist(sretgt, bins = 50)\n",
    "plt.title('Histogram of simulated 10-day return distribution - GARCH-t')\n",
    "plt.show()\n",
    "plt.hist(sretgj, bins = 50)\n",
    "plt.title('Histogram of simulated 10-day return distribution - GJR-GARCH')\n",
    "plt.show()\n",
    "plt.hist(sretgjt, bins = 50)\n",
    "plt.title('Histogram of simulated 10-day return distribution - GJR-GARCH-t')\n",
    "plt.show()\n",
    "\n",
    "#JB tests for Gaussianity of 10-day returns distributions\n",
    "test_stat, p_value = stats.jarque_bera(sretg) \n",
    "test_stat, p_value = stats.jarque_bera(sretgj)\n",
    "test_stat, p_value = stats.jarque_bera(sretgt)\n",
    "test_stat, p_value = stats.jarque_bera(sretgjt)\n",
    "\n",
    "#Skewness and Kurtosis of 10-day returns distributions\n",
    "print([[scipy.stats.skew(sretg), scipy.stats.kurtosis(sretg)], \n",
    "       [scipy.stats.skew(sretgj), scipy.stats.kurtosis(sretgj)], \n",
    "       [scipy.stats.skew(sretgt), scipy.stats.kurtosis(sretgt)], \n",
    "       [scipy.stats.skew(sretgjt), scipy.stats.kurtosis(sretgjt)]])\n",
    "\n",
    "#RiskMetrics\n",
    "sigRM = [np.std(AOr[:250])]\n",
    "for t in range(1, ncis,1):\n",
    "    sigRM.append(np.sqrt(0.06*AOr.iloc[t-1]**2+0.94*sigRM[t-1]**2))\n",
    "sigRMf = sigRM[-1]\n",
    "VaR5RM10 = np.sqrt(10)*sigRM[-1]*stats.norm.ppf(0.05);\n",
    "VaR1RM10 = np.sqrt(10)*sigRM[-1]*stats.norm.ppf(0.01);\n",
    "ES5RM10 = -stats.norm.pdf(stats.norm.ppf(0.05),0,1)/0.05*np.sqrt(10)*sigRMf\n",
    "ES1RM10 = -stats.norm.pdf(stats.norm.ppf(0.01),0,1)/0.01*np.sqrt(10)*sigRMf\n",
    "\n",
    "\n",
    "print('\\nRiskMetrics VaRs', [round(x,4) for x in [VaR1RM10, VaR5RM10]])\n",
    "print('\\nRiskMetrics Expected Shortfalls', [round(x,4) for x in [ES1RM10, ES5RM10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scale of the four distributions is quite comparable. Note that the GJR model induces a small amount of negative skewness in the 10-day return distribution. The skewness for the GJR model are both around -0.47. Also the excess kurtosis for the four distributions are all positive, indicating heavier tails than for a Gaussian random variable. JB tests strongly rejected Gaussianity for all distributions.\n",
    "\n",
    "I used a Monte Carlo sample size of 100000 and it takes about 0.3s to run each time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
